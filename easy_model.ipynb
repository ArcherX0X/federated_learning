{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize indices by class\n",
    "class_indices = defaultdict(list)\n",
    "for idx, (img, label) in enumerate(mnist_data):\n",
    "    class_indices[label].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle within each class\n",
    "for indices in class_indices.values():\n",
    "    random.shuffle(indices)\n",
    "\n",
    "# Generate 10 uneven clients\n",
    "client_data = [[] for _ in range(10)]\n",
    "classes_per_client = [[(i + j) % 10 for j in range(2)] for i in range(10)]  # 2 dominant classes per client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for client_id, cls_list in enumerate(classes_per_client):\n",
    "    for cls in cls_list:\n",
    "        # Assign 600 samples per dominant class\n",
    "        selected = class_indices[cls][:600]\n",
    "        client_data[client_id].extend(selected)\n",
    "        class_indices[cls] = class_indices[cls][600:]\n",
    "\n",
    "    # Optionally add 100 random samples from other classes\n",
    "    for cls in range(10):\n",
    "        if cls not in cls_list and len(class_indices[cls]) >= 100:\n",
    "            selected = class_indices[cls][:100]\n",
    "            client_data[client_id].extend(selected)\n",
    "            class_indices[cls] = class_indices[cls][100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to subsets\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "client_datasets = [Subset(mnist_data, indices) for indices in client_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Federated training loop\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs=1):\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for x, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            output = model(x)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Federated Averaging\n",
    "def federated_avg(models):\n",
    "    global_model = copy.deepcopy(models[0])\n",
    "    for key in global_model.state_dict().keys():\n",
    "        avg = torch.stack([model.state_dict()[key] for model in models], dim=0).mean(dim=0)\n",
    "        global_model.state_dict()[key].copy_(avg)\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1, Test Accuracy: 0.1012\n",
      "Round 2, Test Accuracy: 0.1310\n",
      "Round 3, Test Accuracy: 0.1673\n",
      "Round 4, Test Accuracy: 0.1827\n",
      "Round 5, Test Accuracy: 0.1880\n",
      "Round 6, Test Accuracy: 0.1985\n",
      "Round 7, Test Accuracy: 0.2262\n",
      "Round 8, Test Accuracy: 0.2689\n",
      "Round 9, Test Accuracy: 0.3262\n",
      "Round 10, Test Accuracy: 0.3887\n",
      "Round 11, Test Accuracy: 0.4561\n",
      "Round 12, Test Accuracy: 0.5223\n",
      "Round 13, Test Accuracy: 0.5869\n",
      "Round 14, Test Accuracy: 0.6514\n",
      "Round 15, Test Accuracy: 0.7020\n",
      "Round 16, Test Accuracy: 0.7330\n",
      "Round 17, Test Accuracy: 0.7558\n",
      "Round 18, Test Accuracy: 0.7698\n",
      "Round 19, Test Accuracy: 0.7795\n",
      "Round 20, Test Accuracy: 0.7854\n",
      "Round 21, Test Accuracy: 0.7899\n",
      "Round 22, Test Accuracy: 0.7971\n",
      "Round 23, Test Accuracy: 0.7987\n",
      "Round 24, Test Accuracy: 0.8032\n",
      "Round 25, Test Accuracy: 0.8064\n",
      "Round 26, Test Accuracy: 0.8078\n",
      "Round 27, Test Accuracy: 0.8124\n",
      "Round 28, Test Accuracy: 0.8156\n",
      "Round 29, Test Accuracy: 0.8206\n",
      "Round 30, Test Accuracy: 0.8220\n",
      "Round 31, Test Accuracy: 0.8254\n",
      "Round 32, Test Accuracy: 0.8276\n",
      "Round 33, Test Accuracy: 0.8314\n",
      "Round 34, Test Accuracy: 0.8342\n",
      "Round 35, Test Accuracy: 0.8391\n",
      "Round 36, Test Accuracy: 0.8393\n",
      "Round 37, Test Accuracy: 0.8422\n",
      "Round 38, Test Accuracy: 0.8441\n",
      "Round 39, Test Accuracy: 0.8465\n",
      "Round 40, Test Accuracy: 0.8504\n",
      "Round 41, Test Accuracy: 0.8529\n",
      "Round 42, Test Accuracy: 0.8555\n",
      "Round 43, Test Accuracy: 0.8597\n",
      "Round 44, Test Accuracy: 0.8617\n",
      "Round 45, Test Accuracy: 0.8643\n",
      "Round 46, Test Accuracy: 0.8631\n",
      "Round 47, Test Accuracy: 0.8659\n",
      "Round 48, Test Accuracy: 0.8674\n",
      "Round 49, Test Accuracy: 0.8674\n",
      "Round 50, Test Accuracy: 0.8703\n"
     ]
    }
   ],
   "source": [
    "# Load test set\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)\n",
    "\n",
    "# Federated training\n",
    "rounds = 50\n",
    "global_model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for r in range(rounds):\n",
    "    local_models = []\n",
    "    for client_dataset in client_datasets:\n",
    "        local_model = copy.deepcopy(global_model)\n",
    "        optimizer = optim.SGD(local_model.parameters(), lr=0.01)\n",
    "        loader = DataLoader(client_dataset, batch_size=64, shuffle=True)\n",
    "        train_model(local_model, loader, criterion, optimizer, epochs=1)\n",
    "        local_models.append(local_model)\n",
    "\n",
    "    global_model = federated_avg(local_models)\n",
    "    accuracy = evaluate_model(global_model, test_loader)\n",
    "    print(f'Round {r+1}, Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client dataset indices saved to disk.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create directory to store the client datasets\n",
    "save_dir = \"saved_clients\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save each client dataset (indices only, since MNIST is publicly downloadable)\n",
    "for i, indices in enumerate(client_data):\n",
    "    file_path = os.path.join(save_dir, f\"client_{i}_indices.pt\")\n",
    "    torch.save(indices, file_path)\n",
    "\n",
    "print(\"Client dataset indices saved to disk.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "client_dataset: Each client has its own non-IID subset of MNIST data.\n",
    "\n",
    "local_model: Each client starts with the same global model.\n",
    "\n",
    "train_model(...): The model is trained only on that client's data â€” no sharing of raw data.\n",
    "\n",
    "local_models: All clients' updated models after local training.\n",
    "\n",
    "federated_avg(...): Averages all model parameters (e.g., weights, biases) layer-by-layer.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
